{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d996ed",
   "metadata": {},
   "source": [
    "## Load and preprocess Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84dca6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import pickle\n",
    "\n",
    "#Load and preprocess the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a6bb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0bb4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fc213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the dataset into training and testing sets\n",
    "#USing first 50% for training and remaining 50% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "#Standardize the features\n",
    "#Imortant to scale the features so that every feature get equal \n",
    "#chance of appearing in classification.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a698be",
   "metadata": {},
   "source": [
    "## Create ANN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b71173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the neural network\n",
    "#Topology 5-6-3\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "#Initialize weights and biase for the network\n",
    "np.random.seed(0)\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "bias_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "bias_output = np.zeros(output_size)\n",
    "\n",
    "# softmax function for the output layer\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8407f",
   "metadata": {},
   "source": [
    "## Training ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461ec788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2.3704\n",
      "Epoch 100: Loss 0.8885\n",
      "Epoch 200: Loss 0.7014\n",
      "Epoch 300: Loss 0.6121\n",
      "Epoch 400: Loss 0.5552\n",
      "Epoch 500: Loss 0.5187\n",
      "Epoch 600: Loss 0.4896\n",
      "Epoch 700: Loss 0.4643\n",
      "Epoch 800: Loss 0.4383\n",
      "Epoch 900: Loss 0.4117\n",
      "Epoch 1000: Loss 0.3885\n",
      "Epoch 1100: Loss 0.3681\n",
      "Epoch 1200: Loss 0.3507\n",
      "Epoch 1300: Loss 0.3352\n",
      "Epoch 1400: Loss 0.3210\n",
      "Epoch 1500: Loss 0.3079\n",
      "Epoch 1600: Loss 0.2958\n",
      "Epoch 1700: Loss 0.2847\n",
      "Epoch 1800: Loss 0.2744\n",
      "Epoch 1900: Loss 0.2647\n",
      "Epoch 2000: Loss 0.2556\n",
      "Epoch 2100: Loss 0.2470\n",
      "Epoch 2200: Loss 0.2389\n",
      "Epoch 2300: Loss 0.2312\n",
      "Epoch 2400: Loss 0.2240\n",
      "Epoch 2500: Loss 0.2171\n",
      "Epoch 2600: Loss 0.2105\n",
      "Epoch 2700: Loss 0.2043\n",
      "Epoch 2800: Loss 0.1984\n",
      "Epoch 2900: Loss 0.1928\n",
      "Epoch 3000: Loss 0.1875\n",
      "Epoch 3100: Loss 0.1824\n",
      "Epoch 3200: Loss 0.1776\n",
      "Epoch 3300: Loss 0.1730\n",
      "Epoch 3400: Loss 0.1687\n",
      "Epoch 3500: Loss 0.1646\n",
      "Epoch 3600: Loss 0.1608\n",
      "Epoch 3700: Loss 0.1571\n",
      "Epoch 3800: Loss 0.1536\n",
      "Epoch 3900: Loss 0.1503\n",
      "Epoch 4000: Loss 0.1471\n",
      "Epoch 4100: Loss 0.1441\n",
      "Epoch 4200: Loss 0.1412\n",
      "Epoch 4300: Loss 0.1385\n",
      "Epoch 4400: Loss 0.1359\n",
      "Epoch 4500: Loss 0.1334\n",
      "Epoch 4600: Loss 0.1310\n",
      "Epoch 4700: Loss 0.1288\n",
      "Epoch 4800: Loss 0.1266\n",
      "Epoch 4900: Loss 0.1245\n",
      "Epoch 5000: Loss 0.1225\n",
      "Epoch 5100: Loss 0.1206\n",
      "Epoch 5200: Loss 0.1187\n",
      "Epoch 5300: Loss 0.1170\n",
      "Epoch 5400: Loss 0.1152\n",
      "Epoch 5500: Loss 0.1136\n",
      "Epoch 5600: Loss 0.1120\n",
      "Epoch 5700: Loss 0.1105\n",
      "Epoch 5800: Loss 0.1090\n",
      "Epoch 5900: Loss 0.1076\n",
      "Epoch 6000: Loss 0.1062\n",
      "Epoch 6100: Loss 0.1049\n",
      "Epoch 6200: Loss 0.1036\n",
      "Epoch 6300: Loss 0.1023\n",
      "Epoch 6400: Loss 0.1011\n",
      "Epoch 6500: Loss 0.0999\n",
      "Epoch 6600: Loss 0.0988\n",
      "Epoch 6700: Loss 0.0977\n",
      "Epoch 6800: Loss 0.0966\n",
      "Epoch 6900: Loss 0.0956\n",
      "Epoch 7000: Loss 0.0945\n",
      "Epoch 7100: Loss 0.0935\n",
      "Epoch 7200: Loss 0.0926\n",
      "Epoch 7300: Loss 0.0916\n",
      "Epoch 7400: Loss 0.0907\n",
      "Epoch 7500: Loss 0.0898\n",
      "Epoch 7600: Loss 0.0889\n",
      "Epoch 7700: Loss 0.0881\n",
      "Epoch 7800: Loss 0.0872\n",
      "Epoch 7900: Loss 0.0864\n",
      "Epoch 8000: Loss 0.0856\n",
      "Epoch 8100: Loss 0.0848\n",
      "Epoch 8200: Loss 0.0841\n",
      "Epoch 8300: Loss 0.0833\n",
      "Epoch 8400: Loss 0.0826\n",
      "Epoch 8500: Loss 0.0819\n",
      "Epoch 8600: Loss 0.0812\n",
      "Epoch 8700: Loss 0.0805\n",
      "Epoch 8800: Loss 0.0798\n",
      "Epoch 8900: Loss 0.0792\n",
      "Epoch 9000: Loss 0.0785\n",
      "Epoch 9100: Loss 0.0779\n",
      "Epoch 9200: Loss 0.0773\n",
      "Epoch 9300: Loss 0.0767\n",
      "Epoch 9400: Loss 0.0761\n",
      "Epoch 9500: Loss 0.0755\n",
      "Epoch 9600: Loss 0.0749\n",
      "Epoch 9700: Loss 0.0744\n",
      "Epoch 9800: Loss 0.0738\n",
      "Epoch 9900: Loss 0.0733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "bias_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "bias_output = np.zeros(output_size)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = np.maximum(0, hidden_input)  # ReLU activation function\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    predicted_probabilities = softmax(output_input)\n",
    "\n",
    "    # Calculate the loss\n",
    "    num_samples = X_train.shape[0]\n",
    "    loss = -np.log(predicted_probabilities[range(num_samples), y_train]).mean()\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = predicted_probabilities\n",
    "    d_output[range(num_samples), y_train] -= 1\n",
    "    d_output /= num_samples\n",
    "\n",
    "    d_hidden = np.dot(d_output, weights_hidden_output.T)\n",
    "    d_hidden[hidden_input <= 0] = 0\n",
    "\n",
    "    d_weights_hidden_output = np.dot(hidden_output.T, d_output)\n",
    "    d_bias_output = np.sum(d_output, axis=0)\n",
    "    d_weights_input_hidden = np.dot(X_train.T, d_hidden)\n",
    "    d_bias_hidden = np.sum(d_hidden, axis=0)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    weights_input_hidden -= learning_rate * d_weights_input_hidden\n",
    "    bias_hidden -= learning_rate * d_bias_hidden\n",
    "    weights_hidden_output -= learning_rate * d_weights_hidden_output\n",
    "    bias_output -= learning_rate * d_bias_output\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928017e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7d8bbb",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfd99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test data\n",
    "hidden_input_test = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "hidden_output_test = np.maximum(0, hidden_input_test)\n",
    "output_input_test = np.dot(hidden_output_test, weights_hidden_output) + bias_output\n",
    "predicted_probabilities_test = softmax(output_input_test)\n",
    "predicted_labels_test = np.argmax(predicted_probabilities_test, axis=1)\n",
    "\n",
    "accuracy = (predicted_labels_test == y_test).mean()\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452d0608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cd14e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62e2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"weights_input_hidden\": weights_input_hidden,\n",
    "    \"bias_hidden\": bias_hidden,\n",
    "    \"weights_hidden_output\": weights_hidden_output,\n",
    "    \"bias_output\": bias_output,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82ea01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iris_model_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ff5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "#Load ANN model\n",
    "with open(\"iris_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "#Class labels for Iris dataset\n",
    "class_labels = [\"Iris Setosa\", \"Iris Versicolor\", \"Iris Verginica\"]\n",
    "\n",
    "# Function to make predictions\n",
    "def predict():\n",
    "    try:\n",
    "        # Get user inputs\n",
    "        feature1 = float(entry_feature1.get())\n",
    "        feature2 = float(entry_feature2.get())\n",
    "        feature3 = float(entry_feature3.get())\n",
    "        feature4 = float(entry_feature4.get())\n",
    "\n",
    "        input_features = np.array([feature1, feature2, feature3, feature4]).reshape(1, -1)\n",
    "\n",
    "        weights_input_hidden = model[\"weights_input_hidden\"]\n",
    "        bias_hidden = model[\"bias_hidden\"]\n",
    "        weights_hidden_output = model[\"weights_hidden_output\"]\n",
    "        bias_output = model[\"bias_output\"]\n",
    "\n",
    "        hidden_input = np.dot(input_features, weights_input_hidden) + bias_hidden\n",
    "        hidden_output = np.maximum(0, hidden_input)\n",
    "        output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "        predicted_probabilities = softmax(output_input)\n",
    "\n",
    "        predicted_class_index = np.argmax(predicted_probabilities)\n",
    "        predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "        result_label.config(text=f'Predicted Class: {predicted_class_label}')\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "# Create the tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"ANN Prediction GUI\")\n",
    "window.geometry(\"400x300\")\n",
    "entry_feature1 = tk.Entry(window)\n",
    "entry_feature2 = tk.Entry(window)\n",
    "entry_feature3 = tk.Entry(window)\n",
    "entry_feature4 = tk.Entry(window)\n",
    "\n",
    "entry_feature1.grid(row=0, column=1)\n",
    "entry_feature2.grid(row=1, column=1)\n",
    "entry_feature3.grid(row=2, column=1)\n",
    "entry_feature4.grid(row=3, column=1)\n",
    "\n",
    "label_feature1 = tk.Label(window, text=\"Sepal Length:\")\n",
    "label_feature2 = tk.Label(window, text=\"Sepal Width:\")\n",
    "label_feature3 = tk.Label(window, text=\"Petal Length:\")\n",
    "label_feature4 = tk.Label(window, text=\"Petal Width:\")\n",
    "\n",
    "label_feature1.grid(row=0, column=0)\n",
    "label_feature2.grid(row=1, column=0)\n",
    "label_feature3.grid(row=2, column=0)\n",
    "label_feature4.grid(row=3, column=0)\n",
    "\n",
    "predict_button = tk.Button(window, text=\"Predict\", command=predict)\n",
    "predict_button.grid(row=4, columnspan=2)\n",
    "\n",
    "result_label = tk.Label(window, text=\"\")\n",
    "result_label.grid(row=5, columnspan=2)\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f131a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
