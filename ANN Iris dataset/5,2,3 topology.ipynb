{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d996ed",
   "metadata": {},
   "source": [
    "## Load and preprocess Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84dca6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import pickle\n",
    "\n",
    "#Load and preprocess the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a6bb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0bb4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90fc213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the dataset into training and testing sets\n",
    "#USing first 50% for training and remaining 50% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "#Standardize the features\n",
    "#Imortant to scale the features so that every feature get equal \n",
    "#chance of appearing in classification.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a698be",
   "metadata": {},
   "source": [
    "## Create ANN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67b71173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the neural network\n",
    "#Topology 5-2-3\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "#Initialize weights and biase for the network\n",
    "np.random.seed(0)\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "bias_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "bias_output = np.zeros(output_size)\n",
    "\n",
    "# softmax function for the output layer\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8407f",
   "metadata": {},
   "source": [
    "## Training ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "461ec788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.0063\n",
      "Epoch 100: Loss 0.5732\n",
      "Epoch 200: Loss 0.5234\n",
      "Epoch 300: Loss 0.4871\n",
      "Epoch 400: Loss 0.4593\n",
      "Epoch 500: Loss 0.4372\n",
      "Epoch 600: Loss 0.4194\n",
      "Epoch 700: Loss 0.4046\n",
      "Epoch 800: Loss 0.3918\n",
      "Epoch 900: Loss 0.3810\n",
      "Epoch 1000: Loss 0.3715\n",
      "Epoch 1100: Loss 0.3633\n",
      "Epoch 1200: Loss 0.3559\n",
      "Epoch 1300: Loss 0.3493\n",
      "Epoch 1400: Loss 0.3433\n",
      "Epoch 1500: Loss 0.3378\n",
      "Epoch 1600: Loss 0.3327\n",
      "Epoch 1700: Loss 0.3279\n",
      "Epoch 1800: Loss 0.3234\n",
      "Epoch 1900: Loss 0.3191\n",
      "Epoch 2000: Loss 0.3150\n",
      "Epoch 2100: Loss 0.3110\n",
      "Epoch 2200: Loss 0.3073\n",
      "Epoch 2300: Loss 0.3036\n",
      "Epoch 2400: Loss 0.3000\n",
      "Epoch 2500: Loss 0.2965\n",
      "Epoch 2600: Loss 0.2931\n",
      "Epoch 2700: Loss 0.2898\n",
      "Epoch 2800: Loss 0.2866\n",
      "Epoch 2900: Loss 0.2834\n",
      "Epoch 3000: Loss 0.2804\n",
      "Epoch 3100: Loss 0.2773\n",
      "Epoch 3200: Loss 0.2744\n",
      "Epoch 3300: Loss 0.2715\n",
      "Epoch 3400: Loss 0.2671\n",
      "Epoch 3500: Loss 0.2621\n",
      "Epoch 3600: Loss 0.2571\n",
      "Epoch 3700: Loss 0.2523\n",
      "Epoch 3800: Loss 0.2476\n",
      "Epoch 3900: Loss 0.2431\n",
      "Epoch 4000: Loss 0.2386\n",
      "Epoch 4100: Loss 0.2343\n",
      "Epoch 4200: Loss 0.2301\n",
      "Epoch 4300: Loss 0.2261\n",
      "Epoch 4400: Loss 0.2222\n",
      "Epoch 4500: Loss 0.2184\n",
      "Epoch 4600: Loss 0.2147\n",
      "Epoch 4700: Loss 0.2111\n",
      "Epoch 4800: Loss 0.2077\n",
      "Epoch 4900: Loss 0.2043\n",
      "Epoch 5000: Loss 0.2011\n",
      "Epoch 5100: Loss 0.1980\n",
      "Epoch 5200: Loss 0.1950\n",
      "Epoch 5300: Loss 0.1921\n",
      "Epoch 5400: Loss 0.1894\n",
      "Epoch 5500: Loss 0.1867\n",
      "Epoch 5600: Loss 0.1841\n",
      "Epoch 5700: Loss 0.1817\n",
      "Epoch 5800: Loss 0.1793\n",
      "Epoch 5900: Loss 0.1770\n",
      "Epoch 6000: Loss 0.1748\n",
      "Epoch 6100: Loss 0.1726\n",
      "Epoch 6200: Loss 0.1706\n",
      "Epoch 6300: Loss 0.1687\n",
      "Epoch 6400: Loss 0.1668\n",
      "Epoch 6500: Loss 0.1650\n",
      "Epoch 6600: Loss 0.1633\n",
      "Epoch 6700: Loss 0.1617\n",
      "Epoch 6800: Loss 0.1601\n",
      "Epoch 6900: Loss 0.1586\n",
      "Epoch 7000: Loss 0.1571\n",
      "Epoch 7100: Loss 0.1557\n",
      "Epoch 7200: Loss 0.1544\n",
      "Epoch 7300: Loss 0.1474\n",
      "Epoch 7400: Loss 0.1411\n",
      "Epoch 7500: Loss 0.1352\n",
      "Epoch 7600: Loss 0.1300\n",
      "Epoch 7700: Loss 0.1252\n",
      "Epoch 7800: Loss 0.1209\n",
      "Epoch 7900: Loss 0.1171\n",
      "Epoch 8000: Loss 0.1138\n",
      "Epoch 8100: Loss 0.1109\n",
      "Epoch 8200: Loss 0.1083\n",
      "Epoch 8300: Loss 0.1060\n",
      "Epoch 8400: Loss 0.1040\n",
      "Epoch 8500: Loss 0.1022\n",
      "Epoch 8600: Loss 0.1005\n",
      "Epoch 8700: Loss 0.0989\n",
      "Epoch 8800: Loss 0.0975\n",
      "Epoch 8900: Loss 0.0962\n",
      "Epoch 9000: Loss 0.0949\n",
      "Epoch 9100: Loss 0.0937\n",
      "Epoch 9200: Loss 0.0926\n",
      "Epoch 9300: Loss 0.0915\n",
      "Epoch 9400: Loss 0.0905\n",
      "Epoch 9500: Loss 0.0895\n",
      "Epoch 9600: Loss 0.0885\n",
      "Epoch 9700: Loss 0.0876\n",
      "Epoch 9800: Loss 0.0867\n",
      "Epoch 9900: Loss 0.0859\n"
     ]
    }
   ],
   "source": [
    "#Train the neural network\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass with 3 layers. \n",
    "    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
    "    # usign ReLU activation function in hidden layer (ReLU(x) = max(0, x))\n",
    "    hidden_output = np.maximum(0, hidden_input)  \n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    predicted_probabilities = softmax(output_input)\n",
    "\n",
    "    # Calculate the loss\n",
    "    num_samples = X_train.shape[0]\n",
    "    loss = -np.log(predicted_probabilities[range(num_samples), y_train]).mean()\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = predicted_probabilities\n",
    "    d_output[range(num_samples), y_train] -= 1\n",
    "    d_output /= num_samples\n",
    "\n",
    "    d_hidden = np.dot(d_output, weights_hidden_output.T)\n",
    "    d_hidden[hidden_input <= 0] = 0\n",
    "\n",
    "    d_weights_hidden_output = np.dot(hidden_output.T, d_output)\n",
    "    d_bias_output = np.sum(d_output, axis=0)\n",
    "    d_weights_input_hidden = np.dot(X_train.T, d_hidden)\n",
    "    d_bias_hidden = np.sum(d_hidden, axis=0)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    weights_input_hidden -= learning_rate * d_weights_input_hidden\n",
    "    bias_hidden -= learning_rate * d_bias_hidden\n",
    "    weights_hidden_output -= learning_rate * d_weights_hidden_output\n",
    "    bias_output -= learning_rate * d_bias_output\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0073a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.0850\n",
      "Epoch 100: Loss 0.0842\n",
      "Epoch 200: Loss 0.0834\n",
      "Epoch 300: Loss 0.0827\n",
      "Epoch 400: Loss 0.0820\n",
      "Epoch 500: Loss 0.0813\n",
      "Epoch 600: Loss 0.0806\n",
      "Epoch 700: Loss 0.0799\n",
      "Epoch 800: Loss 0.0792\n",
      "Epoch 900: Loss 0.0786\n",
      "Epoch 1000: Loss 0.0779\n",
      "Epoch 1100: Loss 0.0773\n",
      "Epoch 1200: Loss 0.0767\n",
      "Epoch 1300: Loss 0.0761\n",
      "Epoch 1400: Loss 0.0756\n",
      "Epoch 1500: Loss 0.0750\n",
      "Epoch 1600: Loss 0.0745\n",
      "Epoch 1700: Loss 0.0739\n",
      "Epoch 1800: Loss 0.0734\n",
      "Epoch 1900: Loss 0.0729\n",
      "Epoch 2000: Loss 0.0725\n",
      "Epoch 2100: Loss 0.0720\n",
      "Epoch 2200: Loss 0.0715\n",
      "Epoch 2300: Loss 0.0711\n",
      "Epoch 2400: Loss 0.0706\n",
      "Epoch 2500: Loss 0.0702\n",
      "Epoch 2600: Loss 0.0697\n",
      "Epoch 2700: Loss 0.0693\n",
      "Epoch 2800: Loss 0.0689\n",
      "Epoch 2900: Loss 0.0685\n",
      "Epoch 3000: Loss 0.0681\n",
      "Epoch 3100: Loss 0.0677\n",
      "Epoch 3200: Loss 0.0673\n",
      "Epoch 3300: Loss 0.0670\n",
      "Epoch 3400: Loss 0.0666\n",
      "Epoch 3500: Loss 0.0662\n",
      "Epoch 3600: Loss 0.0659\n",
      "Epoch 3700: Loss 0.0655\n",
      "Epoch 3800: Loss 0.0652\n",
      "Epoch 3900: Loss 0.0649\n",
      "Epoch 4000: Loss 0.0645\n",
      "Epoch 4100: Loss 0.0642\n",
      "Epoch 4200: Loss 0.0639\n",
      "Epoch 4300: Loss 0.0636\n",
      "Epoch 4400: Loss 0.0633\n",
      "Epoch 4500: Loss 0.0630\n",
      "Epoch 4600: Loss 0.0627\n",
      "Epoch 4700: Loss 0.0624\n",
      "Epoch 4800: Loss 0.0621\n",
      "Epoch 4900: Loss 0.0618\n",
      "Epoch 5000: Loss 0.0615\n",
      "Epoch 5100: Loss 0.0612\n",
      "Epoch 5200: Loss 0.0610\n",
      "Epoch 5300: Loss 0.0607\n",
      "Epoch 5400: Loss 0.0604\n",
      "Epoch 5500: Loss 0.0602\n",
      "Epoch 5600: Loss 0.0599\n",
      "Epoch 5700: Loss 0.0597\n",
      "Epoch 5800: Loss 0.0594\n",
      "Epoch 5900: Loss 0.0592\n",
      "Epoch 6000: Loss 0.0589\n",
      "Epoch 6100: Loss 0.0587\n",
      "Epoch 6200: Loss 0.0584\n",
      "Epoch 6300: Loss 0.0582\n",
      "Epoch 6400: Loss 0.0580\n",
      "Epoch 6500: Loss 0.0578\n",
      "Epoch 6600: Loss 0.0575\n",
      "Epoch 6700: Loss 0.0573\n",
      "Epoch 6800: Loss 0.0571\n",
      "Epoch 6900: Loss 0.0569\n",
      "Epoch 7000: Loss 0.0567\n",
      "Epoch 7100: Loss 0.0565\n",
      "Epoch 7200: Loss 0.0562\n",
      "Epoch 7300: Loss 0.0560\n",
      "Epoch 7400: Loss 0.0558\n",
      "Epoch 7500: Loss 0.0556\n",
      "Epoch 7600: Loss 0.0554\n",
      "Epoch 7700: Loss 0.0553\n",
      "Epoch 7800: Loss 0.0551\n",
      "Epoch 7900: Loss 0.0549\n",
      "Epoch 8000: Loss 0.0547\n",
      "Epoch 8100: Loss 0.0545\n",
      "Epoch 8200: Loss 0.0543\n",
      "Epoch 8300: Loss 0.0541\n",
      "Epoch 8400: Loss 0.0540\n",
      "Epoch 8500: Loss 0.0538\n",
      "Epoch 8600: Loss 0.0536\n",
      "Epoch 8700: Loss 0.0534\n",
      "Epoch 8800: Loss 0.0533\n",
      "Epoch 8900: Loss 0.0531\n",
      "Epoch 9000: Loss 0.0529\n",
      "Epoch 9100: Loss 0.0528\n",
      "Epoch 9200: Loss 0.0526\n",
      "Epoch 9300: Loss 0.0524\n",
      "Epoch 9400: Loss 0.0523\n",
      "Epoch 9500: Loss 0.0521\n",
      "Epoch 9600: Loss 0.0520\n",
      "Epoch 9700: Loss 0.0518\n",
      "Epoch 9800: Loss 0.0517\n",
      "Epoch 9900: Loss 0.0515\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5777ce92",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dfd99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test data\n",
    "hidden_input_test = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "hidden_output_test = np.maximum(0, hidden_input_test)\n",
    "output_input_test = np.dot(hidden_output_test, weights_hidden_output) + bias_output\n",
    "predicted_probabilities_test = softmax(output_input_test)\n",
    "predicted_labels_test = np.argmax(predicted_probabilities_test, axis=1)\n",
    "\n",
    "accuracy = (predicted_labels_test == y_test).mean()\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d987b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a54c088b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b62e2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"weights_input_hidden\": weights_input_hidden,\n",
    "    \"bias_hidden\": bias_hidden,\n",
    "    \"weights_hidden_output\": weights_hidden_output,\n",
    "    \"bias_output\": bias_output,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c82ea01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iris_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6c1ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "#Load ANN model\n",
    "with open(\"iris_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "#Class labels for Iris dataset\n",
    "class_labels = [\"Iris Setosa\", \"Iris Versicolor\", \"Iris Verginica\"]\n",
    "\n",
    "# Function to make predictions\n",
    "def predict():\n",
    "    try:\n",
    "        # Get user inputs\n",
    "        feature1 = float(entry_feature1.get())\n",
    "        feature2 = float(entry_feature2.get())\n",
    "        feature3 = float(entry_feature3.get())\n",
    "        feature4 = float(entry_feature4.get())\n",
    "\n",
    "        input_features = np.array([feature1, feature2, feature3, feature4]).reshape(1, -1)\n",
    "\n",
    "        weights_input_hidden = model[\"weights_input_hidden\"]\n",
    "        bias_hidden = model[\"bias_hidden\"]\n",
    "        weights_hidden_output = model[\"weights_hidden_output\"]\n",
    "        bias_output = model[\"bias_output\"]\n",
    "\n",
    "        hidden_input = np.dot(input_features, weights_input_hidden) + bias_hidden\n",
    "        hidden_output = np.maximum(0, hidden_input)\n",
    "        output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "        predicted_probabilities = softmax(output_input)\n",
    "\n",
    "        predicted_class_index = np.argmax(predicted_probabilities)\n",
    "        predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "        result_label.config(text=f'Predicted Class: {predicted_class_label}')\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "# Create the tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"ANN Prediction GUI\")\n",
    "window.geometry(\"400x300\")\n",
    "entry_feature1 = tk.Entry(window)\n",
    "entry_feature2 = tk.Entry(window)\n",
    "entry_feature3 = tk.Entry(window)\n",
    "entry_feature4 = tk.Entry(window)\n",
    "\n",
    "entry_feature1.grid(row=0, column=1)\n",
    "entry_feature2.grid(row=1, column=1)\n",
    "entry_feature3.grid(row=2, column=1)\n",
    "entry_feature4.grid(row=3, column=1)\n",
    "\n",
    "label_feature1 = tk.Label(window, text=\"Sepal Length:\")\n",
    "label_feature2 = tk.Label(window, text=\"Sepal Width:\")\n",
    "label_feature3 = tk.Label(window, text=\"Petal Length:\")\n",
    "label_feature4 = tk.Label(window, text=\"Petal Width:\")\n",
    "\n",
    "label_feature1.grid(row=0, column=0)\n",
    "label_feature2.grid(row=1, column=0)\n",
    "label_feature3.grid(row=2, column=0)\n",
    "label_feature4.grid(row=3, column=0)\n",
    "\n",
    "predict_button = tk.Button(window, text=\"Predict\", command=predict)\n",
    "predict_button.grid(row=4, columnspan=2)\n",
    "\n",
    "result_label = tk.Label(window, text=\"\")\n",
    "result_label.grid(row=5, columnspan=2)\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c1ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
